# Why Agents Don't Watch the Clock

**Diagnosing Temporal Grounding Failures in Reinforcement Learning**

[![arXiv](https://img.shields.io/badge/arXiv-2602.xxxx-b31b1b.svg)](https://arxiv.org/abs/2602.xxxx)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üìù Quick Links

- **[Paper (PDF)](paper.pdf)** - Full arXiv preprint
- **[Code](#code)** - Environment and training scripts
- **[Results](#results)** - Pre-computed results and visualizations

---

## üéØ Core Finding

**We diagnose a fundamental challenge in training time-aware agents:**

> Standard RL reward structures create incentives for agents to discover *time-agnostic shortcuts* that avoid the difficult cognitive task of time estimation.

Our experiments reveal that both PPO and PPO+PRM agents achieve **0% success rate** in our ChronoEnv benchmark, discovering "submit immediately" strategies rather than learning to check time.

---

## üìä Quick Stats

| Metric | Value |
|--------|-------|
| Environment | ChronoEnv v2.0 (Time-Critical Regime) |
| Fixed Strategy Success | 45% (fails on work > 100 tasks) |
| PPO Success Rate | 0% (1000 episodes) |
| PPO Queries | 0.00 |
| PPO+PRM Success Rate | 0% (1000 episodes) |
| PRM Queries | 7.47¬±1.2 (preliminary, 3 seeds) |

---

## üöÄ Getting Started

### Installation

```bash
# Requirements
pip install torch gymnasium matplotlib numpy

# Clone repository
git clone https://github.com/autratec/Temporal-Grounding-Failures.git
cd Temporal-Grounding-Failures

# Environment is in: code/chrono_env_v2.py
# Training scripts are in: code/
```

### Run Training

```bash
# PPO baseline (3 seeds, 1000 episodes)
python3 code/train_complete.py --method ppo --seeds 0,1,2 --episodes 1000

# PPO+PRM (3 seeds, 1000 episodes)
python3 code/train_complete.py --method prm --seeds 0,1,2 --episodes 1000

# Both side by side
python3 code/train_complete.py --method both --seeds 0,1,2 --episodes 1000
```

### Run Diagnostics

```bash
# Query cost sensitivity analysis
python3 code/run_sensitivity.py

# Temporal debugger (visualize agent behavior)
python3 code/temporal_debugger.py
```

---

## üìÅ Repository Structure

```
Temporal-Grounding-Failures/
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ chrono_env_v2.py           # ChronoEnv v2.0 (Time-Critical Regime)
‚îÇ   ‚îú‚îÄ‚îÄ chrono_env_time_penalty.py # Alternative reward structure
‚îÇ   ‚îú‚îÄ‚îÄ prm_temporal.py            # Temporal Regret Module
‚îÇ   ‚îú‚îÄ‚îÄ ppo_agent.py               # PPO agent implementation
‚îÇ   ‚îú‚îÄ‚îÄ train_complete.py          # Full training framework
‚îÇ   ‚îî‚îÄ‚îÄ __pycache__/
‚îú‚îÄ‚îÄ arxiv/
‚îÇ   ‚îú‚îÄ‚îÄ paper.tex                  # arXiv paper source
‚îÇ   ‚îú‚îÄ‚îÄ template.tex               # Paper template
‚îÇ   ‚îú‚îÄ‚îÄ generate_figures.py        # Figure generation script
‚îÇ   ‚îú‚îÄ‚îÄ run_sensitivity.py         # Query cost sensitivity analysis
‚îÇ   ‚îú‚îÄ‚îÄ temporal_debugger.py       # Agent behavior visualization
‚îÇ   ‚îî‚îÄ‚îÄ figures/                   # Generated figures
‚îÇ       ‚îú‚îÄ‚îÄ fixed_policy_analysis.pdf/png
‚îÇ       ‚îú‚îÄ‚îÄ learning_curves.pdf/png
‚îÇ       ‚îú‚îÄ‚îÄ query_cost_sensitivity.pdf/png
‚îÇ       ‚îú‚îÄ‚îÄ reward_sensitivity.pdf/png
‚îÇ       ‚îú‚îÄ‚îÄ debugger_comparison.png
‚îÇ       ‚îú‚îÄ‚îÄ ppo_traj_seed_*.png    # PPO trajectories
‚îÇ       ‚îî‚îÄ‚îÄ prm_traj_seed_*.png    # PRM trajectories
‚îî‚îÄ‚îÄ README.md                      # This file
```

---

## üìä Results

### Learning Curves

![Learning Curves](figures/learning_curves.png)

*Both PPO and PPO+PRM fail to learn. Both achieve 0% success rate.*

### Query Cost Sensitivity

![Query Cost Sensitivity](figures/query_cost_sensitivity.png)

*Changing query cost doesn't improve success rate. Agents discover safe shortcuts.*

### Fixed Policy Analysis

![Fixed Policy Analysis](figures/fixed_policy_analysis.png)

*Fixed 100-step strategy achieves only 45% success - proves environment difficulty.*

---

## üîç Key Findings

### 1. ChronoEnv Successfully Eliminates Fixed Strategies

- Fixed 100-step policy: 45% success (fails on work > 100 tasks)
- This proves the environment forces time awareness

### 2. PPO Discovers Reward Hacking

- 0% success rate across 1000 episodes
- 0 queries - agent learns "submit immediately" strategy
- This is a time-agnostic shortcut

### 3. PRM Provides Some Guidance

- PRM agents query more (7.47¬±1.2 queries vs 0.00)
- But still 0% success rate
- Signal too weak compared to dominant "submit immediately" reward

---

## üß† Why This Matters

### The Problem

Agents frequently fail to track physical time, completing tasks at strange times. This suggests a fundamental challenge in training time-aware agents.

### The Insight

Standard RL reward structures create incentives for agents to avoid the difficult cognitive task of time estimation. They discover "time-agnostic shortcuts" instead.

### The Solution (Future Work)

- **Curriculum Learning:** Start easy, gradually increase difficulty
- **Hybrid Architecture:** Combine PRM with explicit time representation
- **Inverse Temporal Regret:** Reward not checking when unnecessary

---

## üìö Citation

```bibtex
@article{temporal-grounding-failures,
  title={Why Agents Don't Watch the Clock: Diagnosing Temporal Grounding Failures in Reinforcement Learning},
  author={Xu, Zhe},
  journal={arXiv preprint arXiv:2602.xxxx},
  year={2026}
}
```

---

## ü§ù Contributing

Contributions welcome! Areas for improvement:

- [ ] Add more environment variations
- [ ] Implement curriculum learning baseline
- [ ] Add human study component
- [ ] Extend to multi-agent settings

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details.

---

## üôè Acknowledgements

We thank the RL community for feedback on temporal grounding challenges.

---

*This repository contains the arXiv preprint "Why Agents Don't Watch the Clock: Diagnosing Temporal Grounding Failures in Reinforcement Learning".*
