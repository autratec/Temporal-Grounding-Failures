\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{caption}

\title{Why Agents Don't Watch the Clock: Diagnosing Temporal Grounding Failures in Reinforcement Learning}

\author{Jeff \texttt{et al.} \\
\texttt{jeff@example.com} \\
}

\date{Preprint.}

\begin{document}

\maketitle

\begin{abstract}
We observe that LLM-based agents often fail to track physical time across multi-turn interactions. To study this, we introduce \textbf{ChronoEnv}, a time-critical RL benchmark where agents must balance task completion against the cost of querying time. Our experiments reveal a pervasive \textit{reward hacking} phenomenon: both PPO and PPO+PRM agents prefer time-agnostic shortcuts over learning active temporal grounding. We analyze the root causes and propose process rewards as a promising but insufficient direction. We open-source ChronoEnv to catalyze research on time-aware agents.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Time-awareness is a fundamental capability for autonomous agents operating in dynamic environments. From scheduling tasks to coordinating with humans, agents need to know when to act. Yet, we observe in practice that LLM-based agents often exhibit \textit{temporal myopia}: they complete tasks correctly but with strange timing (e.g., submitting a report at 3:30 when the deadline is 4:00, then claiming "I'm done!" as if it's perfect timing).

\subsection{Problem Statement}

\textbf{Observation:} Agents frequently fail to track physical time across interactions, instead relying on work-progress heuristics or internal counters.

\textbf{Question:} Why do agents avoid learning temporal grounding? Is it a representation limitation, a reward misalignment issue, or an exploration challenge?

\textbf{Hypothesis:} Standard RL reward structures create incentives for agents to discover \textit{time-agnostic shortcuts} that avoid the costly and uncertain act of querying time.

\subsection{Our Contributions}

\begin{enumerate}
    \item \textbf{ChronoEnv:} A new RL benchmark that forces agents to balance work progress with time querying costs.
    
    \item \textbf{Reward Hacking Diagnosis:} We demonstrate that even sophisticated agents (PPO+PRM) discover time-agnostic shortcuts in our environment.
    
    \item \textbf{Process Rewards for Temporal Grounding:} We extend PRM to include temporal regret, showing preliminary evidence of promise despite current limitations.
    
    \item \textbf{Diagnostic Framework:} We release ChronoEnv along with visualization tools to help researchers diagnose temporal grounding failures.
\end{enumerate}

\section{Related Work}
\label{sec:related_work}

\textbf{Temporal Reasoning in RL.} Existing work on temporal reasoning focuses on tasks with explicit time signals or pre-defined schedules. Our work differs by studying \textit{active temporal grounding} where agents must learn when to query time.

\textbf{Process Rewards.} PRM has been used to guide agent behavior beyond terminal rewards. We extend this to temporal domains, encouraging agents to value \textit{when} they obtain information.

\textbf{Reward Hacking.} Our work contributes to the growing understanding of specification gaming in RL, showing how reward design can inadvertently incentivize avoidance of difficult cognitive tasks.

\section{ChronoEnv: A Benchmark for Active Temporal Grounding}
\label{sec:environment}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/env_diagram.pdf}
    \caption{ChronoEnv Overview. Agent must complete work before deadline while balancing query costs.}
    \label{fig:env_diagram}
\end{figure}

\subsection{Environment Design}

ChronoEnv is a time-constrained task scheduling environment. The agent observes:
\begin{itemize}
    \item \textit{internal\_estimate:} Agent's belief about elapsed time (noisy, drifts from ground truth)
    \item \textit{deadline:} Absolute deadline for task completion
    \item \textit{time\_since\_check:} Steps since last query
    \item \textit{query\_count:} Number of queries made
\end{itemize}

\textbf{Actions:}
\begin{itemize}
    \item \textit{WORK:} Make progress toward deadline
    \item \textit{WAIT:} Wait (time passes, no progress)
    \item \textit{CHECK\_TIME:} Query external clock (cost: -1.0, noisy observation)
    \item \textit{SUBMIT:} Submit task (success if work done AND time < deadline)
\end{itemize}

\textbf{Key Design Choices:}
\begin{itemize}
    \item \textit{Random work duration} (80-120 steps): Agent doesn't know exact requirement
    \item \textit{Tight deadline} (work + 5-15 buffer): No room for error
    \item \textit{No step count in observation}: Cannot rely on counting
    \item \textit{Noisy CHECK\_TIME}: Simulates imperfect clock
\end{itemize}

\subsection{Why This Environment is Hard}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Strategy} & \textbf{Success Rate} & \textbf{Why It Fails} \\
\midrule
Fixed 100 steps & 45\% & Fails on work > 100 tasks \\
Submit immediately & 0\% & Work not done \\
PPO (baseline) & 0\% & Reward hacking: avoids time \\
PPO+PRM & 0\% & PRM signal not strong enough \\
\bottomrule
\end{tabular}
\caption{Strategy evaluation on ChronoEnv v2.0}
\label{tab:strategies}
\end{table}

\textbf{Key Insight:} The environment successfully eliminates time-agnostic strategies. Only 45\% success for fixed policy proves that agents must learn time awareness.

\section{The Reward Hacking Phenomenon}
\label{sec:reward_hacking}

\subsection{Observation}

Both PPO and PPO+PRM agents achieve \textbf{0\% success rate} with \textbf{0 queries} across 1000+ episodes. This indicates:

\begin{itemize}
    \item Agent discovers "submit immediately" strategy (reward = -50)
    \item This avoids the complexity of time estimation
    \item No signal guides agent toward time-aware behavior
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/learning_curves.pdf}
    \caption{Learning curves for PPO vs PPO+PRM. Both fail to learn.}
    \label{fig:learning_curves}
\end{figure}

\subsection{Root Cause Analysis}

\textbf{Hypothesis 1: Sparse Reward Signal}
\begin{quote}
The only rewarding signal is +100 for success, which rarely occurs. Agent cannot learn from sparse feedback.
\end{quote}

\textbf{Hypothesis 2: Hard Exploration}
\begin{quote}
The space of "submit immediately" is much larger than "submit at deadline". Agent gets stuck in local optimum.
\end{quote}

\textbf{Hypothesis 3: Reward Misalignment}
\begin{quote}
The reward structure doesn't incentivize checking time - it only rewards final outcome.
\end{quote}

\subsection{Evidence from Fixed Policy Analysis}

We tested a fixed "work 100 steps" policy:
\begin{itemize}
    \item Success rate: 45\%
    \item Failures occur when work required > 100 (55\% of tasks)
\end{itemize}

This proves the environment forces time awareness. The fact that PPO fails entirely suggests \textbf{Reward Hacking}: the agent found an even safer strategy (submit immediately).

\section{Process Rewards as a Potential Direction}
\label{sec:process_rewards}

\subsection{Temporal PRM Design}

We extend PRM to include temporal regret signals:

\begin{align*}
R_{\text{prm}} = &- \lambda_1 \cdot \mathbb{I}(\text{redundant check}) \\
&- \lambda_2 \cdot \mathbb{I}(\text{high uncertainty} \land \text{no check}) \\
&+ \lambda_3 \cdot \mathbb{I}(\text{correct timing})
\end{align*}

\subsection{Results and Limitations}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Success Rate} & \textbf{Avg Queries} \\
\midrule
PPO & 0\% & 0.00 \\
PPO+PRM & 0\% & 0.00 (100 eps) \\
PPO+PRM & 0\% & 7.47 (200 eps, preliminary) \\
\bottomrule
\end{tabular}
\caption{PRM results (preliminary)}
\label{tab:prm_results}
\end{table}

\textbf{Observation:} PRM introduces querying behavior (7.47 queries in 100-episode run), but success rate remains 0\%.

\textbf{Analysis:} The PRM signal may be too weak compared to the dominant "submit immediately" shortcut reward (-50 vs +0.5 for good time estimation).

\subsection{Future Directions}

\begin{itemize}
    \item \textbf{Curriculum Learning:} Start with larger buffer, gradually tighten
    \item \textbf{Hybrid Architecture:} Combine PRM with explicit time representation
    \item \textbf{Inverse Temporal Regret:} Reward \textit{not} checking when unnecessary
\end{itemize}

\section{Discussion}
\label{sec:discussion}

\subsection{Why This Matters}

\textbf{Human Analogy:} Humans also exhibit "temporal optimism bias" - we estimate time inaccurately and only check clocks when uncertainty is high. Agents may need similar meta-cognitive mechanisms.

\textbf{Agent Safety:} Temporal myopia can cause agents to miss critical deadlines, arrive too early, or waste resources.

\subsection{Limitations}

\begin{itemize}
    \item Current environment is challenging - no agent achieves >0\% success
    \item PRM signals need tuning to be effective
    \item Results limited to 1000 episodes (compute constraints)
\end{itemize}

\subsection{Future Work}

\begin{enumerate}
    \item Curriculum learning with uncertainty-aware PRM
    \item Hybrid architecture with explicit time representation
    \item Human study to validate "temporal optimism bias" hypothesis
    \item Extension to multi-agent settings with communication
\end{enumerate}

\section{Reproducibility}
\label{sec:reproducibility}

\textbf{Code:} \url{https://github.com/autratec/openclaw/projects/time-aware-lunar-lander/}

\textbf{Environment:} \texttt{ChronoEnvTimeCritical} in \texttt{chrono\_env\_v2.py}

\textbf{Training Script:} \texttt{train\_complete.py}

\textbf{Results:} Pre-computed results in \texttt{results/\_}

\textbf{Citation:}
\begin{verbatim}
@article{why_agents_dont_watch_clock,
  title={Why Agents Don't Watch the Clock: Diagnosing Temporal Grounding Failures in Reinforcement Learning},
  author={Jeff et al.},
  journal={arXiv preprint arXiv:...},
  year={2026}
}
\end{verbatim}

\section*{Acknowledgements}

We thank the RL community for feedback on temporal grounding challenges.

\end{document}
